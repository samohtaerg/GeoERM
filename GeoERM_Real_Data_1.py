# -*- coding: utf-8 -*-
"""HPC R10

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19PuPggsgyNgL-qHpLmVu64OVryd3stAW

### GeoERM
"""

# Implementation of different MTL methods
import numpy as np
import torch
import torch.optim as optim
from sklearn.linear_model import LinearRegression, LogisticRegression
import logging

def R(X):
    Q, _ = torch.qr(X)
    return Q

def polar_retraction(X):
    U, _, Vt = torch.linalg.svd(X, full_matrices=False)
    return U @ Vt

# Orthogonal Projection1
def Proj(X, U):
    print(X.shape)
    print(U.shape)
    return U - X @ (X.T @ U + U.T @ X) / 2

# Orthogonal Projection2
def projection(X, U):
    # X^T * U
    XtU = torch.matmul(X.transpose(-2, -1), U)
    symXtU = (XtU + XtU.transpose(-2, -1)) / 2

    # Projection: U - X * symXtU
    Up = U - torch.matmul(X, symXtU)
    return Up

def column_norm(A):
    norm_A = np.zeros(A.shape[1])
    for j in range(A.shape[1]):
        norm_A[j] = np.linalg.norm(A[:, j])
    return(norm_A)

def initialize_stiefel_matrix(p, r):
    random_matrix = np.random.randn(p, r)
    Q, _ = np.linalg.qr(random_matrix)
    return Q[:, :r]

def initialize_stiefel_tensor(T, p, r):
    A_hat = np.zeros((T, p, r))
    for t in range(T):
        random_matrix = np.random.randn(p, r)
        Q, _ = np.linalg.qr(random_matrix)
        A_hat[t] = Q[:, :r]
    return A_hat

## penalized GeoERM
def GeoERM(data, r = 3, T1 = 1, T2 = 1, R = None, r_bar = None, lr = 0.01, max_iter = 20, C1 = 1, C2 = 1,
            delta = 0.05, adaptive = False, info = False, tol = 1e-6, link = 'linear'):
    if info:
        print("pERM starts running...", flush = True)

    T = len(data)
    n = np.array([x.shape[0] for (x,y) in data])
    p = data[0][0].shape[1]
    n_total = np.sum(n)

    ## initialization
    x = np.zeros((n_total, p))
    y = np.zeros(n_total)

    # calculate sample indices for each task
    task_range = []
    start_index = 0
    for t in range(T):
        task_range.append(range(start_index, start_index+n[t]))
        start_index += n[t]

    # stack the x and y arrays
    for t in range(T):
        x[task_range[t], :] = data[t][0]
        y[task_range[t]] = data[t][1]

    ## r adaptive or not
    if (adaptive == True):
        # single-task linear regression
        beta_hat_single_task = np.zeros((p, T))
        if link == 'linear':
            for t in range(T):
                beta_hat_single_task[:, t] = LinearRegression(fit_intercept = False).fit(x[task_range[t], :], y[task_range[t]]).coef_
        elif link == 'logistic':
            for t in range(T):
                beta_hat_single_task[:, t] = LogisticRegression(fit_intercept = False).fit(x[task_range[t], :], y[task_range[t]]).coef_

        norm_each_task = column_norm(beta_hat_single_task)
        if R is None:
            R = np.median(norm_each_task)*2

        for t in range(T):
            if (norm_each_task[t] > R):
                beta_hat_single_task[:, t] = beta_hat_single_task[:, t]/norm_each_task[t]*R

        # set up threshold
        if r_bar is None:
            r_bar = x.shape[1]
        threshold = T1*np.sqrt((p+np.log(T))/np.max(n)) + T2*R*(r_bar**(-3/4))
        r = max(np.where(np.linalg.svd(beta_hat_single_task/np.sqrt(T))[1] > threshold)[0])+1
        if info:
            print('Selected r = ' + str(r))

    # initialization
    y = torch.tensor(y, requires_grad=False)
    x = torch.tensor(x, requires_grad=False)
    # A_hat = np.random.rand(T, p, r)#, dtype ='float64')
    # A_bar = np.random.rand(p, r)#, dtype='float64')
    A_hat = initialize_stiefel_tensor(T, p, r)
    A_bar = initialize_stiefel_matrix(p, r)
    A_bar[0:r, 0:r] = np.identity(r,dtype='float64')

    for t in range(T):
        A_hat[t, 0:r, 0:r] = np.identity(r)

    theta_hat = np.zeros((r, T))

    # transform arrays to tensors
    # A_hat = torch.tensor(A_hat, requires_grad=True)
    A_bar = torch.tensor(A_bar, requires_grad=True, dtype=torch.float64)
    A_hat = torch.tensor(A_hat, requires_grad=True, dtype=torch.float64)
    theta_hat = torch.tensor(theta_hat, requires_grad=True, dtype=torch.float64)

    ## Step 1
    lam = np.sqrt(r*(p+np.log(T)))*C1

    if link == 'linear':
        def ftotal(A, theta, A_bar):
            s = 0
            for t in range(T):
                s = s + 1/(2*n_total)*torch.dot(y[task_range[t]] - x[task_range[t], :] @ A[t, :, :] @ theta[:, t], y[task_range[t]]
                                     - x[task_range[t], :] @ A[t, :, :] @ theta[:, t]) + lam*np.sqrt(n[t])/n_total*torch.linalg.svd(A[t, :, :] @ torch.linalg.inv(A[t, :, :].T @ A[t, :, :]) @ A[t, :, :].T - A_bar @ torch.linalg.inv(A_bar.T @ A_bar) @ A_bar.T)[1][0]
            return(s)
    elif link == 'logistic':
        def ftotal(A, theta, A_bar):
            s = 0
            for t in range(T):
                logits = torch.matmul(x[task_range[t], :], A[t, :, :] @ theta[:, t])
                s = s + 1/n_total*torch.dot(1-y[task_range[t]], logits) + 1/n_total*torch.sum(torch.log(1+torch.exp(-logits))) + lam*np.sqrt(n[t])/n_total*torch.linalg.svd(A[t, :, :] @ torch.linalg.inv(A[t, :, :].T @ A[t, :, :]) @ A[t, :, :].T - A_bar @ torch.linalg.inv(A_bar.T @ A_bar) @ A_bar.T)[1][0]

            return(s)

    optimizer = optim.Adam([A_hat, theta_hat, A_bar], lr=lr)

    loss_last = 1e8
    for i in range(max_iter):
        # Zero the gradients
        optimizer.zero_grad()

        # Compute the loss (sum of the largest singular values)
        loss = ftotal(A_hat, theta_hat, A_bar)

        # Backward pass to compute gradients
        loss.backward()
        # print(A_bar.grad)

        # Projection on gradients
        projected_A_hat_grad = projection(A_hat, A_hat.grad)
        projected_A_bar_grad = projection(A_bar, A_bar.grad)

        # Update the gradients to their projected versions
        with torch.no_grad():
            A_hat.grad.copy_(projected_A_hat_grad)
            A_bar.grad.copy_(projected_A_bar_grad)

        # Update the matrices with Adam
        optimizer.step()
        # print(A_bar.grad)

        with torch.no_grad():
            A_hat.copy_(polar_retraction(A_hat))
            A_bar.copy_(polar_retraction(A_bar))
        # with torch.no_grad():
        #     A_hat.copy_(R(A_hat))
        #     A_bar.copy_(R(A_bar))

        # Loss
        if info:
            if (i + 1) % 100 == 0:
                print("Iteration {}/{}, Loss: {}".format(i+1, max_iter, loss.item()), flush=True)

        # Early Stop
        if abs(loss_last - loss.item()) / loss.item() <= tol:
            if info:
                print("Already converged. Stopped early.", flush=True)
            break
        loss_last = loss.item()

    beta_hat_step1 = torch.zeros(p, T, dtype=torch.float64)
    for t in range(T):
        beta_hat_step1[:, t] = A_hat[t, :, :] @ theta_hat[:, t]

    beta_hat_step1 = beta_hat_step1.detach()
    if info:
        print("Step 1 is completed.\n", flush=True)

    ## Step 2
    gamma = np.sqrt(p+np.log(T))*C2
    beta = torch.zeros(p, T, requires_grad = True, dtype=torch.float64)

    if link == 'linear':
        def ftotal2(beta):
            s = 0
            for t in range(T):
                s = s + 1/(2*n[t])*torch.dot(y[task_range[t]] - x[task_range[t], :] @ beta[:, t], y[task_range[t]] - x[task_range[t], :] @ beta[:, t]) + gamma/np.sqrt(n[t])*torch.norm(beta[:, t] - beta_hat_step1[:, t])
            return(s)
    elif link == 'logistic':
        def ftotal2(beta):
            s = 0
            for t in range(T):
                logits = torch.matmul(x[task_range[t], :], beta[:, t])
                s = s + 1/n[t]*torch.dot(1-y[task_range[t]], logits) + 1/n[t]*torch.sum(torch.log(1+torch.exp(-logits))) + gamma/np.sqrt(n[t])*torch.norm(beta[:, t] - beta_hat_step1[:, t])
            return(s)


    optimizer2 = optim.Adam([beta], lr=lr)
    loss_last = 1e8
    for i in range(max_iter):
        # Zero the gradients
        optimizer2.zero_grad()

        # Compute the loss (sum of the largest singular values)
        loss2 = ftotal2(beta)

        # Backward pass to compute gradients
        loss2.backward()

        # Update the matrices
        optimizer2.step()

        # Print the loss every 100 iterations
        if info:
            if (i + 1) % 100 == 0:
                print("Iteration {}/{}, Loss: {}".format(i+1, max_iter, loss2.item()), flush = True)
        if abs(loss_last-loss2.item())/loss2.item() <= tol:
            if info:
                print("Already converged. Stopped early.", flush = True)
            break
        loss_last = loss2.item()

    beta_hat_step1 = beta_hat_step1.numpy()
    beta_hat_step2 = beta.detach().numpy()
    if info:
        print("Step 2 is completed.\n", flush = True)

    if info:
        print("GeoERM stops running...", flush = True)

    return({"step1": beta_hat_step1, "step2": beta_hat_step2})

"""# Real Data Result

### Preload funcs
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Real-data analysis in Section 5.2
"""

import numpy as np
import sys, os
import pickle
import torch
import csv
import argparse

# Set root path for HPC environment
root_path = "/scratch/ac10374/RL_TL_MTL/"

# Add root path to sys.path to ensure all submodules can be found
sys.path.append(root_path)

# Add additional paths
path1 = root_path
path2 = root_path + "benchmarks/ARMUL"
path3 = root_path + "benchmarks/AdaptRep"
path4 = root_path + "benchmarks/GLasso"
sys.path.append(os.path.join(root_path, "benchmarks/ARMUL/ARMUL"))

sys.path.append(os.path.join(os.path.dirname(path1)))
sys.path.append(os.path.join(os.path.dirname(path2)))
sys.path.append(os.path.join(os.path.dirname(path3)))
sys.path.append(os.path.join(os.path.dirname(path4)))

# Import modules
from mtl_func_torch import pERM, ERM, spectral, single_task_LR, pooled_LR
from funcs import prediction, all_classification_error
from ARMUL.ARMUL import ARMUL_blackbox

"""## Problem Setup"""

# Load the read data
load_path = '/scratch/ac10374/har_standardized.pkl'

# Preprocess
with open(load_path, 'rb') as f:
    (x_pca, y, subject) = pickle.load(f)


# assign data to each task according to subject value, split data into training/test sets
train_data = []
test_data = []
test_ratio = 0.5
for t in np.unique(subject):
    index = np.where(subject == t)[0]
    test_index = np.random.choice(index, size = int(np.floor(index.size*test_ratio)), replace = False)
    train_index = np.setdiff1d(index, test_index)

    train_data.append((x_pca[train_index,:], y[train_index]))
    test_data.append((x_pca[test_index,:], y[test_index]))

"""## Run Result"""

import os
import sys
import numpy as np
import torch
import csv

def run_sim():
    # Initialize error matrix
    est_error_S = np.zeros((7, 30))

    # Run pERM
    print('Running pERM...', flush=True)
    beta_hat_pERM = pERM(data=train_data, r=r, C1=1, C2=0.5, adaptive=False, link='logistic', max_iter=2000, info=False)
    y_pred_pERM = prediction(beta_hat_pERM['step2'], test_data)
    print('pERM completed.', flush=True)

    # Run GeoERM
    print('Running GeoERM...', flush=True)
    beta_hat_GeoERM = GeoERM(data=train_data, r=r, C1=1, C2=0.5, adaptive=False, link='logistic', max_iter=2000, info=False)
    y_pred_GeoERM = prediction(beta_hat_GeoERM['step2'], test_data)
    print('GeoERM completed.', flush=True)

    # Run ARMUL
    beta_hat_ARMUL = ARMUL_blackbox(train_data, r, eta=0.1, L=10, n_fold=5, seed=seed, link='logistic', c_max=5)
    y_pred_ARMUL = prediction(beta_hat_ARMUL, test_data)

    # Single-task regression
    beta_hat_single_task = single_task_LR(train_data, link='logistic')
    y_pred_single_task = prediction(beta_hat_single_task, test_data)

    # Pooled regression
    beta_hat_pooled = pooled_LR(train_data, link='logistic')
    y_pred_pooled = prediction(beta_hat_pooled, test_data)

    # Run ERM
    beta_hat_ERM = ERM(data=train_data, r=r, info=False, link='logistic')
    y_pred_ERM = prediction(beta_hat_ERM, test_data)

    # Spectral method
    beta_hat_spectral = spectral(data=train_data, r=r, C2=0.5, T1=0.25, T2=0.5, adaptive=False, link='logistic', info=True, q=0.05)
    y_pred_spectral = prediction(beta_hat_spectral['step2'], test_data)

    # Record estimation errors
    est_error_S[0, :] = all_classification_error(y_pred_single_task, test_data)
    est_error_S[1, :] = all_classification_error(y_pred_pooled, test_data)
    est_error_S[2, :] = all_classification_error(y_pred_ERM, test_data)
    est_error_S[3, :] = all_classification_error(y_pred_ARMUL, test_data)
    est_error_S[4, :] = all_classification_error(y_pred_pERM, test_data)
    est_error_S[5, :] = all_classification_error(y_pred_spectral, test_data)
    est_error_S[6, :] = all_classification_error(y_pred_GeoERM, test_data)

    return est_error_S

if __name__ == '__main__':
    # Get task ID and random seed
    task_id = int(os.getenv('SLURM_ARRAY_TASK_ID', 0))  # Default to 0 if undefined
    seed = task_id

    print(f'Running with seed {seed}', flush=True)

    # Set random seed
    np.random.seed(seed)
    torch.manual_seed(seed)

    # Set intrinsic dimension
    r = 10
    print(f"Using r value: {r}", flush=True)

    # Specify output directory
    output_dir = '/scratch/ac10374/hpcr10/'

    # Ensure output directory exists
    os.makedirs(output_dir, exist_ok=True)

    # Use SLURM_ARRAY_TASK_ID as unique identifier
    proc_id = task_id
    file_path = os.path.join(output_dir, f'realdata_{r}_{proc_id}.csv')

    # Check if output file exists
    if os.path.exists(file_path):
        print(f"File {file_path} exists. Exiting script.", flush=True)
        sys.exit(0)
    else:
        print(f"File {file_path} does not exist. Proceeding.", flush=True)

    # Run simulation and get error estimates
    est_error = run_sim()

    # Save results to file
    with open(file_path, 'w', newline='') as csvfile:
        writer = csv.writer(csvfile, delimiter=',')
        writer.writerows(est_error)

    print(f"Results saved to {file_path}", flush=True)