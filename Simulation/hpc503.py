# -*- coding: utf-8 -*-
"""HPC503

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lX2dpeFikWLBR5-V9kt1rBG-AzUQSnKg
"""

# -*- coding: utf-8 -*-
# Helper Functions"""

# Useful functions
import os
import json
from numpy.linalg import norm
import numpy as np

def avg_distance(beta_hat, beta):
    s = 0
    T = beta.shape[1]
    for t in range(T):
        s = s + norm(beta_hat[:, t]-beta[:, t])/T
    return(s)

def max_distance(beta_hat, beta, S = None):
    if S is None:
        T = beta.shape[1]
        S = [i for i in range(0, T)]

    s = np.zeros(T)
    for t in S:
        s[t] = norm(beta_hat[:, t]-beta[:, t])
    return(max(s))

def all_distance(beta_hat, beta):
    T = beta.shape[1]
    s = np.zeros(T)
    for t in range(T):
        s[t] = norm(beta_hat[:, t]-beta[:, t])
    return(s)


def prediction(beta_hat, test_data):
    y_pred = []
    t = 0
    for (x, y) in test_data:
        y_pred.append((x @ beta_hat[:, t] > 0).astype(int))
        t += 1
    return(y_pred)

def all_classification_error(y_pred, test_data):
    error = np.zeros(len(test_data))
    for t in range(len(test_data)):
        error[t] = np.mean(y_pred[t] != test_data[t][1])
    return(error)

"""# Data Generation"""

# Data generation function for simulations

import numpy as np
from scipy.linalg import sqrtm

# --------------------------------------
def generate_data(n = 100, p = 50, r = 5, T = 50, h = 0, epsilon = 0, link = 'linear', H = 2):
    num_outlier = int(np.floor(epsilon*T))
    S = np.sort(np.random.choice(range(T), size = T-num_outlier, replace = False))
    Sc = np.setdiff1d(range(T), S)
    theta = np.random.uniform(low = -H, high = H, size = T*r).reshape(r, T)
    R = np.random.normal(0, 1, p*p).reshape((p,p))
    A_center = (np.linalg.svd(R)[0])[0:r, ]
    A_center = A_center.T # p*r matrix
    A = np.zeros((T, p, r))
    beta = np.zeros((p, T))

    # generate beta in S
    for t in S:
        Delta_A = np.zeros((p, r))
        Delta_A[0:r, 0:r] = np.random.uniform(low=-h,high=h,size=1)*np.identity(r)
        A[t, :, :] = A_center + Delta_A
        sqrt_ATA = sqrtm(A[t,:,:].T@A[t,:,:])
        sqrt_ATA[abs(sqrt_ATA)<=1e-10] = 0
        A[t, :, :] = A[t, :, :] @ sqrt_ATA
        #QR retraction to make sure A on the manifold.
        Q, R = np.linalg.qr(A[t, :, :])
        A[t, :, :] = Q
        beta[:, t] = A[t, :, :] @ theta[:, t]
        beta[:, t] = A[t, :, :] @ theta[:, t]

    # generate beta outside S
    if num_outlier > 0:
        beta_outlier = np.random.uniform(low = -3, high = 3, size = num_outlier*p).reshape(p, num_outlier)
        beta[:, Sc] = beta_outlier

    # data generation
    train_data = []
    for t in range(T):
        if t in S:
            x = np.random.normal(0, 1, n*p).reshape((n,p))
        else:
            x = np.random.normal(0, 2, n*p).reshape((n,p))

        if link == 'linear':
            y = x @ beta[:, t] + np.random.normal(0, 1, n)
        elif link == 'logistic':
            prob = 1/(1+np.exp(-x @ beta[:, t]))
            y = np.random.binomial(1, prob)

        train_data.append((x, y))

    output_dict = {'data': train_data,
                   'beta': beta,
                   'S': S}
    return(output_dict)

"""#penalized ERM

"""

# Implementation of different MTL methods


import numpy as np
import torch
import torch.optim as optim
from sklearn.linear_model import LinearRegression, LogisticRegression

def column_norm(A):
    norm_A = np.zeros(A.shape[1])
    for j in range(A.shape[1]):
        norm_A[j] = np.linalg.norm(A[:, j])
    return(norm_A)


## penalized ERM (Algorithm 1)
def pERM(data, r = 3, T1 = 1, T2 = 1, R = None, r_bar = None, lr = 0.01, max_iter = 2000, C1 = 1, C2 = 1,
            delta = 0.05, adaptive = False, info = False, tol = 1e-6, link = 'linear'):
    if info:
        print("pERM starts running...", flush = True)

    T = len(data)
    n = np.array([x.shape[0] for (x,y) in data])
    p = data[0][0].shape[1]
    n_total = np.sum(n)

    ## initialization
    x = np.zeros((n_total, p))
    y = np.zeros(n_total)

    # calculate sample indices for each task
    task_range = []
    start_index = 0
    for t in range(T):
        task_range.append(range(start_index, start_index+n[t]))
        start_index += n[t]

    # stack the x and y arrays
    for t in range(T):
        x[task_range[t], :] = data[t][0]
        y[task_range[t]] = data[t][1]

    ## r adaptive or not
    if (adaptive == True):
        # single-task linear regression
        beta_hat_single_task = np.zeros((p, T))
        if link == 'linear':
            for t in range(T):
                beta_hat_single_task[:, t] = LinearRegression(fit_intercept = False).fit(x[task_range[t], :], y[task_range[t]]).coef_
        elif link == 'logistic':
            for t in range(T):
                beta_hat_single_task[:, t] = LogisticRegression(fit_intercept = False).fit(x[task_range[t], :], y[task_range[t]]).coef_

        norm_each_task = column_norm(beta_hat_single_task)
        if R is None:
            R = np.median(norm_each_task)*2

        for t in range(T):
            if (norm_each_task[t] > R):
                beta_hat_single_task[:, t] = beta_hat_single_task[:, t]/norm_each_task[t]*R

        # set up threshold
        if r_bar is None:
            r_bar = x.shape[1]
        threshold = T1*np.sqrt((p+np.log(T))/np.max(n)) + T2*R*(r_bar**(-3/4))
        r = max(np.where(np.linalg.svd(beta_hat_single_task/np.sqrt(T))[1] > threshold)[0])+1
        if info:
            print('Selected r = ' + str(r))

    # initialization
    y = torch.tensor(y, requires_grad=False)
    x = torch.tensor(x, requires_grad=False)
    A_hat = np.zeros((T, p, r), dtype ='float64')
    A_bar = np.zeros((p, r), dtype='float64')
    A_bar[0:r, 0:r] = np.identity(r,dtype='float64')

    for t in range(T):
        A_hat[t, 0:r, 0:r] = np.identity(r)

    theta_hat = np.zeros((r, T))

    # transform arrays to tensors
    A_hat = torch.tensor(A_hat, requires_grad=True)
    A_bar = torch.tensor(A_bar, requires_grad=True)
    theta_hat = torch.tensor(theta_hat, requires_grad=True)

    ## Step 1
    lam = np.sqrt(r*(p+np.log(T)))*C1

    if link == 'linear':
        def ftotal(A, theta, A_bar):
            s = 0
            for t in range(T):
                s = s + 1/(2*n_total)*torch.dot(y[task_range[t]] - x[task_range[t], :] @ A[t, :, :] @ theta[:, t], y[task_range[t]]
                                     - x[task_range[t], :] @ A[t, :, :] @ theta[:, t]) + lam*np.sqrt(n[t])/n_total*torch.linalg.svd(A[t, :, :] @ torch.linalg.inv(A[t, :, :].T @ A[t, :, :]) @ A[t, :, :].T - A_bar @ torch.linalg.inv(A_bar.T @ A_bar) @ A_bar.T)[1][0]
            return(s)
    elif link == 'logistic':
        def ftotal(A, theta, A_bar):
            s = 0
            for t in range(T):
                logits = torch.matmul(x[task_range[t], :], A[t, :, :] @ theta[:, t])
                s = s + 1/n_total*torch.dot(1-y[task_range[t]], logits) + 1/n_total*torch.sum(torch.log(1+torch.exp(-logits))) + lam*np.sqrt(n[t])/n_total*torch.linalg.svd(A[t, :, :] @ torch.linalg.inv(A[t, :, :].T @ A[t, :, :]) @ A[t, :, :].T - A_bar @ torch.linalg.inv(A_bar.T @ A_bar) @ A_bar.T)[1][0]

            return(s)


    optimizer = optim.Adam([A_hat, theta_hat, A_bar], lr=lr)

    loss_last = 1e8
    for i in range(max_iter):
        # Zero the gradients
        optimizer.zero_grad()

        # Compute the loss (sum of the largest singular values)
        loss = ftotal(A_hat, theta_hat, A_bar)

        # Backward pass to compute gradients
        loss.backward()

        # Update the matrices
        optimizer.step()

        # Print the loss every 100 iterations
        if info:
            if (i + 1) % 100 == 0:
                print("Iteration {}/{}, Loss: {}".format(i+1, max_iter, loss.item()), flush = True)
        if abs(loss_last-loss.item())/loss.item() <= tol:
            if info:
                print("Already converged. Stopped early.", flush = True)
            break
        loss_last = loss.item()


    beta_hat_step1 = torch.zeros(p, T, dtype=torch.float64)
    for t in range(T):
        beta_hat_step1[:, t] = A_hat[t,:,:] @ theta_hat[:, t]

    beta_hat_step1 = beta_hat_step1.detach()
    if info:
        print("Step 1 is completed.\n", flush = True)


    ## Step 2
    gamma = np.sqrt(p+np.log(T))*C2
    beta = torch.zeros(p, T, requires_grad = True, dtype=torch.float64)

    if link == 'linear':
        def ftotal2(beta):
            s = 0
            for t in range(T):
                s = s + 1/(2*n[t])*torch.dot(y[task_range[t]] - x[task_range[t], :] @ beta[:, t], y[task_range[t]] - x[task_range[t], :] @ beta[:, t]) + gamma/np.sqrt(n[t])*torch.norm(beta[:, t] - beta_hat_step1[:, t])
            return(s)
    elif link == 'logistic':
        def ftotal2(beta):
            s = 0
            for t in range(T):
                logits = torch.matmul(x[task_range[t], :], beta[:, t])
                s = s + 1/n[t]*torch.dot(1-y[task_range[t]], logits) + 1/n[t]*torch.sum(torch.log(1+torch.exp(-logits))) + gamma/np.sqrt(n[t])*torch.norm(beta[:, t] - beta_hat_step1[:, t])
            return(s)


    optimizer2 = optim.Adam([beta], lr=lr)
    loss_last = 1e8
    for i in range(max_iter):
        # Zero the gradients
        optimizer2.zero_grad()

        # Compute the loss (sum of the largest singular values)
        loss2 = ftotal2(beta)

        # Backward pass to compute gradients
        loss2.backward()

        # Update the matrices
        optimizer2.step()

        # Print the loss every 100 iterations
        if info:
            if (i + 1) % 100 == 0:
                print("Iteration {}/{}, Loss: {}".format(i+1, max_iter, loss2.item()), flush = True)
        if abs(loss_last-loss2.item())/loss2.item() <= tol:
            if info:
                print("Already converged. Stopped early.", flush = True)
            break
        loss_last = loss2.item()

    beta_hat_step1 = beta_hat_step1.numpy()
    beta_hat_step2 = beta.detach().numpy()
    if info:
        print("Step 2 is completed.\n", flush = True)

    if info:
        print("pERM stops running...", flush = True)

    return({"step1": beta_hat_step1, "step2": beta_hat_step2})

"""# AdaptRep

"""

import numpy as np
import torch
from torch.optim import LBFGS, SGD
from tqdm.notebook import trange


def generate_inputs(d, k, n, T, eps):
    top_block_samples = np.sqrt(eps) * torch.randn(T, n, d-k)
    bottom_block_samples = torch.randn(T, n, k)
    return torch.cat([top_block_samples, bottom_block_samples], dim=-1)

def generate_weights(d, k, T, eps):
    dominant, residual = torch.randn(T, k), torch.randn(T, k)
    dominant /= torch.norm(dominant, dim=-1, keepdim=True)
    residual /= torch.norm(residual, dim=-1, keepdim=True)
    return torch.cat([np.sqrt(1/(2*eps)) * dominant, torch.zeros((T, d-2*k)), residual], axis=-1)

def generate_labels(inputs, predictors, noise_stddev):
    means = torch.matmul(inputs, predictors[:, :, None])[:, :, 0]
    return means + noise_stddev * torch.randn(*means.shape)


def train_loop(opt_vars, optimizer, loss_fn, projection, datasets, train_params, plot_losses=False):
    """
    opt_vars: list of PyTorch variables
    optimizer: pytorch.optim
    loss_fn: takes (opt_vars, X, y)
    projection: takes opt_vars, applies changes to .data only
    datasets: list containing two tensors of shapes [(T, n, d), (T, n)]
              first tensor is inputs, second is labels, T is number of parallel datasets
    train_params: {batch_size, num_batches, train_desc}
    """
    dataset_size = datasets[0].shape[1]
    batch_size = train_params["batch_size"]
    num_batches_per_pass = int(np.ceil(dataset_size / batch_size))

    with torch.no_grad():
        projection(opt_vars)

    best_opt_vars, best_loss = [opt_var.data.clone() for opt_var in opt_vars], np.inf
    losses = []

    # for num_batch in trange(train_params["num_batches"], leave=False, desc=train_params["train_desc"]):
    #     batch_idx = num_batch % num_batches_per_pass
    for num_batch in range(train_params["num_batches"]):
        batch_idx = num_batch % num_batches_per_pass

        # Check if this is a new_pass - if so, shuffle the data
        if batch_idx == 0:
            perm = torch.randperm(dataset_size)
            datasets = [datasets[0][:, perm], datasets[1][:, perm]]

        start_idx, end_idx = batch_idx * batch_size, (batch_idx + 1) * batch_size
        batch_X, batch_y = datasets[0][:, start_idx:end_idx], datasets[1][:, start_idx:end_idx]

        optimizer.zero_grad()
        loss_fn(opt_vars, batch_X, batch_y).backward()
        optimizer.step(lambda: loss_fn(opt_vars, batch_X, batch_y))

        with torch.no_grad():
            projection(opt_vars)
            cur_loss = loss_fn(opt_vars, datasets[0], datasets[1]).item()
            if cur_loss <= best_loss:
                best_opt_vars = [opt_var.data.clone() for opt_var in opt_vars]
                best_loss = cur_loss
            losses.append(cur_loss)

    return best_opt_vars



def prep_adaptrep_training(d, k, T, n, eps, noise_stddev):
    B_train = torch.randn(d, k, requires_grad=True)
    W_train = torch.randn(k, T, requires_grad=True)
    Delta_train = torch.randn(d, T, requires_grad=True)
    opt_vars = [B_train, W_train, Delta_train]
    optimizer = LBFGS(opt_vars, lr=0.1, history_size=5)

    def loss_fn(opt_vars, inputs, labels):
        B_train, W_train, Delta_train = opt_vars
        weights_train = (torch.matmul(B_train, W_train) + Delta_train).T
        predictions = torch.matmul(inputs, weights_train[:, :, None])[:, :, 0]
        mse = torch.mean(torch.square(labels - predictions))
        regularization = 1/(4*T) * torch.sum(torch.square(torch.matmul(B_train.T, B_train) - torch.matmul(W_train, W_train.T)))
        delta_reg = noise_stddev * (np.sqrt(k * d / (n * T)) + np.sqrt(k / n)) * torch.mean(torch.norm(Delta_train, dim=0))
        return mse + regularization + delta_reg

    def create_projection(C0):
        def projection(opt_vars):
            B_train, W_train, Delta_train = opt_vars
            W_train.data /= torch.max(torch.norm(W_train, dim=0, keepdim=True) / np.sqrt(C0 * np.sqrt(k / T)), torch.tensor([1.]))
            v_randn = (1e-2)*torch.randn(k, T)
            W_train.data = torch.where(torch.isnan(W_train.data), v_randn, W_train.data)
            W_train.data = torch.where(torch.isinf(W_train.data), v_randn, W_train.data)
            W_train.data /= torch.max(torch.svd(W_train, compute_uv=False)[1][0] / np.sqrt(C0 * np.sqrt(T / k)), torch.tensor([1.]))
            v_randn_b = (1e-2)*torch.randn(d, k)
            B_train.data = torch.where(torch.isnan(B_train.data), v_randn_b, B_train.data)
            B_train.data = torch.where(torch.isinf(B_train.data), v_randn_b, B_train.data)
            B_train.data /= torch.max(torch.svd(B_train, compute_uv=False)[1][0] / np.sqrt(C0 * np.sqrt(T / k)), torch.tensor([1.]))
            Delta_train.data /= torch.max(torch.norm(Delta_train, dim=0, keepdim=True) / 2, torch.tensor([1]).float())
        return projection

    return opt_vars, optimizer, loss_fn, create_projection(3)


def find_adversary(B_learned):
    d, k = B_learned.shape
    orth_opt = torch.cat([torch.eye(k), torch.zeros(d-k, k)], axis=0)
    delta_opt = torch.cat([torch.zeros(d-k, k), torch.eye(k)], axis=0)

    orth_learned = torch.svd(B_learned, some=True)[0]
    projector_learned = torch.matmul(orth_learned, orth_learned.T)
    proj_perp_learned = torch.eye(projector_learned.shape[0]) - projector_learned

    opt_res = torch.matmul(proj_perp_learned, orth_opt)
    delta_res = torch.matmul(proj_perp_learned, delta_opt)

    return torch.matmul(orth_opt, torch.svd(opt_res, some=True)[2][:, :1]), \
           torch.matmul(delta_opt, torch.svd(delta_res, some=True)[2][:, :1])

def prep_training_w_rep(B_learned, d, k, T, n, eps, noise_stddev):
    W_train = torch.randn(k, T, requires_grad=True)
    Delta_train = torch.randn(d, T, requires_grad=True)
    opt_vars = [W_train, Delta_train]
    optimizer = SGD(opt_vars, lr=0.01)

    def loss_fn(opt_vars, inputs, labels):
        W_train, Delta_train = opt_vars
        weights_train = (torch.matmul(B_learned, W_train) + Delta_train).T
        predictions = torch.matmul(inputs, weights_train[:, :, None])[:, :, 0]
        mse = torch.sum(torch.mean(torch.square(labels - predictions), dim=-1))
        regularization = (2 * k / np.sqrt(n)) * torch.sum(torch.norm(Delta_train, dim=0))
        return mse + regularization

    def projection(opt_vars):
        pass

    return opt_vars, optimizer, loss_fn, projection

def prep_baseline(d, k, T, n, eps, noise_stddev):
    Delta_train = torch.randn(d, T, requires_grad=True)
    opt_vars = [Delta_train]
    optimizer = SGD(opt_vars, lr=0.1)

    def loss_fn(opt_vars, inputs, labels):
        Delta_train = opt_vars[0]
        weights_train = Delta_train.T
        predictions = torch.matmul(inputs, weights_train[:, :, None])[:, :, 0]
        return torch.sum(torch.mean(torch.square(labels - predictions), dim=-1))

    def projection(opt_vars):
        pass

    return opt_vars, optimizer, loss_fn, projection


def AdaptRep(data, r, num_batches = 200, batch_size = 2):
    T = len(data)
    p = data[0][0].shape[1]
    n = data[0][0].shape[0]
    x = np.zeros((T, n, p))
    y = np.zeros((T, n))
    noise_stddev = 1
    eps = r / p
    for t in range(T):
        x[t, :, :] = data[t][0]
        y[t, :] = data[t][1]

    y0 = torch.tensor(y, requires_grad=False, dtype=torch.float32)
    x0 = torch.tensor(x, requires_grad=False, dtype=torch.float32)


    opt_vars, optimizer, loss_fn, projector = prep_adaptrep_training(p, r, T, n, eps, noise_stddev)
    opt_vars = train_loop(opt_vars, optimizer, loss_fn, projector, (x0, y0), {"num_batches": num_batches, "batch_size": batch_size, "train_desc": "Training by adaptation"})

    beta_hat_AdaptRep = (opt_vars[0] @ opt_vars[1] + opt_vars[2]).detach().numpy()
    return(beta_hat_AdaptRep)

"""# Other Models"""

## ERM (the same representation across tasks)
def ERM(data, r, eta = 0.05, delta = 0.05, max_iter = 2000, lr = 0.01, info = False, tol = 1e-6, link = 'linear'):
    if info:
        print("ERM starts running...", flush = True)

    T = len(data)
    n = np.array([x.shape[0] for (x,y) in data])
    p = data[0][0].shape[1]
    n_total = np.sum(n)

    ## initialization
    x = np.zeros((n_total, p))
    y = np.zeros(n_total)

    # calculate sample indices for each task
    task_range = []
    start_index = 0
    for t in range(T):
        task_range.append(range(start_index, start_index+n[t]))
        start_index += n[t]

    # stack the x and y arrays
    for t in range(T):
        x[task_range[t], :] = data[t][0]
        y[task_range[t]] = data[t][1]


    y = torch.tensor(y, requires_grad=False)
    x = torch.tensor(x, requires_grad=False)
    A_hat = np.zeros((p, r), dtype ='float64')
    for t in range(T):
        A_hat[0:r, 0:r] = np.identity(r)

    # transform arrays to tensors
    A_hat = torch.tensor(A_hat, requires_grad=True)
    theta_hat = torch.zeros(r, T, requires_grad=True, dtype=torch.float64)

    if link == 'linear':
        def ftotal(A, theta):
            s = 0
            for t in range(T):
                s = s + 1/(2*n_total)*torch.dot(y[task_range[t]] - x[task_range[t], :] @ A @ theta[:, t], y[task_range[t]] - x[task_range[t], :] @ A @ theta[:, t])
            return(s)
    elif link == 'logistic':
        def ftotal(A, theta):
            s = 0
            for t in range(T):
                logits = torch.matmul(x[task_range[t], :], A @ theta[:, t])
                s = s + 1/n_total*torch.dot(1-y[task_range[t]], logits) + 1/n_total*torch.sum(torch.log(1+torch.exp(-logits)))

            return(s)

    optimizer = optim.Adam([A_hat, theta_hat], lr=lr)
    loss_last = 1e8
    for i in range(max_iter):
        # Zero the gradients
        optimizer.zero_grad()

        # Compute the loss (sum of the largest singular values)
        loss = ftotal(A_hat, theta_hat)

        # Backward pass to compute gradients
        loss.backward()

        # Update the matrices
        optimizer.step()

        # Print the loss every 100 iterations
        if info:
            if (i + 1) % 100 == 0:
                print("Iteration {}/{}, Loss: {}".format(i+1, max_iter, loss.item()), flush = True)

        if abs(loss_last-loss.item())/loss.item() <= tol:
            if info:
                print("Already converged. Stopped early.", flush = True)
            break
        loss_last = loss.item()

    beta_hat = torch.zeros(p, T, dtype=torch.float64)
    for t in range(T):
        beta_hat[:, t] = A_hat @ theta_hat[:, t]

    beta_hat = beta_hat.detach().numpy()
    if info:
        print("ERM stops running...", flush = True)

    return(beta_hat)

## Single-task regression
def single_task_LR(data, link = 'linear'):
    T = len(data)
    p = data[0][0].shape[1]
    beta_hat = np.zeros((p, T))
    if link == 'linear':
        for t in range(T):
            beta_hat[:, t] = LinearRegression(fit_intercept = False).fit(data[t][0], data[t][1]).coef_
    elif link == 'logistic':
        for t in range(T):
            beta_hat[:, t] = LogisticRegression(fit_intercept = False).fit(data[t][0], data[t][1]).coef_
    return(beta_hat)


## Pooled regression
def pooled_LR(data, link = 'linear'):
    T = len(data)
    p = data[0][0].shape[1]
    x_all = np.empty((0, p))
    y_all = np.empty(0)
    for (x, y) in data:
        x_all = np.concatenate((x_all, x))
        y_all = np.concatenate((y_all, y))

    beta_hat = np.zeros((p, T))
    if link == 'linear':
        beta_fit = LinearRegression(fit_intercept = False).fit(x_all, y_all).coef_
    elif link == 'logistic':
        beta_fit = LogisticRegression(fit_intercept = False).fit(x_all, y_all).coef_

    for t in range(T):
        beta_hat[:, t] = beta_fit
    return(beta_hat)


## Estimation of r (Algorithm 3)
def select_r(data, T1 = 0.5, T2 = 0.25, R = None, r_bar = None, q = 0.05, epsilon_bar = 0.05, link = 'linear'):
    n = np.array([x.shape[0] for (x,y) in data])
    T = len(data)
    p = data[0][0].shape[1]
    beta_hat_single_task = np.zeros((p, T))
    # var_est = np.zeros(T)
    if link == 'linear':
        for t in range(T):
            beta_hat_single_task[:, t] = LinearRegression(fit_intercept = False).fit(data[t][0], data[t][1]).coef_
    elif link == 'logistic':
        for t in range(T):
            beta_hat_single_task[:, t] = LogisticRegression(fit_intercept = False).fit(data[t][0], data[t][1]).coef_

    norm_each_task = column_norm(beta_hat_single_task)
    if R is None:
        R = np.quantile(norm_each_task, q)

    for t in range(T):
        if (norm_each_task[t] > R):
            beta_hat_single_task[:, t] = beta_hat_single_task[:, t]/norm_each_task[t]*R

    # set up threshold
    if r_bar is None:
        r_bar = p
    threshold = T1*np.sqrt((p+np.log(T))/np.max(n)) + T2*R*np.sqrt(epsilon_bar)
    sigval = np.linalg.svd(beta_hat_single_task/np.sqrt(T))[1]
    if len(np.where(sigval > threshold)[0]) > 0:
        r = max(np.where(sigval > threshold)[0])+1
        print('Threshold = ' + str(threshold) + ', selected r = ' + str(r))
        return(r)
    else:
        print('No r is selected. Too large threshold.')
        return(None)


## Spectral method (Algorithm 2)
def spectral(data, r, C2 = 1, T1 = 1, T2 = 1, R = None, r_bar = None, lr = 0.01, max_iter = 2000, info = False, adaptive = False, tol = 1e-6, link = 'linear', q = 0):
    if info:
        print("spectral starts running...")
    T = len(data)
    n = np.array([x.shape[0] for (x,y) in data])
    p = data[0][0].shape[1]
    n_total = np.sum(n)

    ## initialization
    x = np.zeros((n_total, p))
    y = np.zeros(n_total)

    # calculate sample indices for each task
    task_range = []
    start_index = 0
    for t in range(T):
        task_range.append(range(start_index, start_index+n[t]))
        start_index += n[t]

    # stack the x and y arrays
    for t in range(T):
        x[task_range[t], :] = data[t][0]
        y[task_range[t]] = data[t][1]

    ## r adaptive or not
    if (adaptive == True):
        # single-task linear regression
        beta_hat_single_task = np.zeros((p, T))
        if link == 'linear':
            for t in range(T):
                beta_hat_single_task[:, t] = LinearRegression(fit_intercept = False).fit(x[task_range[t], :], y[task_range[t]]).coef_
        elif link == 'logistic':
            for t in range(T):
                beta_hat_single_task[:, t] = LogisticRegression(fit_intercept = False).fit(x[task_range[t], :], y[task_range[t]]).coef_

        norm_each_task = column_norm(beta_hat_single_task)
        if R is None:
            R = np.median(norm_each_task)*2

        for t in range(T):
            if (norm_each_task[t] > R):
                beta_hat_single_task[:, t] = beta_hat_single_task[:, t]/norm_each_task[t]*R

        # set up threshold
        if r_bar is None:
            r_bar = x.shape[1]
        threshold = T1*np.sqrt((p+np.log(T))/np.max(n)) + T2*R*(r_bar**(-3/4))
        r = max(np.where(np.linalg.svd(beta_hat_single_task/np.sqrt(T))[1] > threshold)[0])+1
        if info:
            print('Threshold = ' + str(threshold) + ', selected r = ' + str(r))


    # single-task linear regression
    B = np.zeros((p, T))
    if link == 'linear':
        for t in range(T):
            B[:, t] = LinearRegression(fit_intercept = False).fit(x[task_range[t], :], y[task_range[t]]).coef_
    elif link == 'logistic':
        for t in range(T):
            B[:, t] = LogisticRegression(fit_intercept = False).fit(x[task_range[t], :], y[task_range[t]]).coef_


    # truncation for robustness
    norm_each_task = np.zeros(T)
    for t in range(T):
        norm_each_task[t] = np.linalg.norm(B[:, t])
    if q > 0:
        R = np.quantile(norm_each_task, 1-q)
        for t in range(T):
            B[:, t] = B[:, t]/norm_each_task[t]*R


    ## Step 1
    theta_hat = np.zeros((r, T))
    beta_hat_step1 = np.zeros((p, T))

    # calculate the central representation
    A_hat = np.linalg.svd(B)[0][:, 0:r]

    # calculate the theta estimate based on A_hat
    if link == 'linear':
        for t in range(T):
            theta_hat[:, t] = LinearRegression(fit_intercept = False).fit(x[task_range[t], :] @ A_hat, y[task_range[t]]).coef_
    elif link == 'logistic':
        for t in range(T):
            theta_hat[:, t] = LogisticRegression(fit_intercept = False).fit(x[task_range[t], :] @ A_hat, y[task_range[t]]).coef_

    # calculate the estimate of coef
    for t in range(T):
        beta_hat_step1[:, t] = A_hat @ theta_hat[:, t]

    beta_hat_step1 = torch.tensor(beta_hat_step1, requires_grad=False)

    if info:
        print("Step 1 is completed.\n", flush = True)

    ## Step 2: Biased regularization
    # torch initialization
    y = torch.tensor(y, requires_grad=False)
    x = torch.tensor(x, requires_grad=False)

    beta = torch.zeros((p, T), requires_grad=True, dtype=torch.float64)
    gamma = np.sqrt(p+np.log(T))*C2
    if link == 'linear':
        def ftotal2(beta):
           s = 0
           for t in range(T):
               s = s + 1/(2*n[t])*torch.dot(y[task_range[t]] - x[task_range[t], :] @ beta[:, t], y[task_range[t]] - x[task_range[t], :] @ beta[:, t]) + gamma/np.sqrt(n[t])*torch.norm(beta[:, t] - beta_hat_step1[:, t])
           return(s)
    elif link == 'logistic':
        def ftotal2(beta):
           s = 0
           for t in range(T):
               logits = torch.matmul(x[task_range[t], :], beta[:, t])
               s = s + 1/n[t]*torch.dot(1-y[task_range[t]], logits) + 1/n[t]*torch.sum(torch.log(1+torch.exp(-logits))) + gamma/np.sqrt(n[t])*torch.norm(beta[:, t] - beta_hat_step1[:, t])
           return(s)

    optimizer2 = optim.Adam([beta], lr=lr)
    loss_last = 1e8
    for i in range(max_iter):
        # Zero the gradients
        optimizer2.zero_grad()

        # Compute the loss (sum of the largest singular values)
        loss2 = ftotal2(beta)

        # Backward pass to compute gradients
        loss2.backward()

        # Update the matrices
        optimizer2.step()

        # Print the loss every 100 iterations
        if info:
            if (i + 1) % 100 == 0:
                print("Iteration {}/{}, Loss: {}".format(i+1, max_iter, loss2.item()), flush = True)
        if abs(loss_last-loss2.item())/loss2.item() <= tol:
            if info:
                print("Already converged. Stopped early.", flush = True)
            break
        loss_last = loss2.item()

    beta_hat_step1 = beta_hat_step1.numpy()
    beta_hat_step2 = beta.detach().numpy()
    if info:
        print("Step 2 is completed.\n", flush = True)
    if info:
        print("spectral stops running...", flush = True)

    return({"step1": beta_hat_step1, "step2": beta_hat_step2})


## Method-of-moments
def MoM(data, r):
    T = len(data)
    n = np.array([x.shape[0] for (x,y) in data])
    p = data[0][0].shape[1]
    n_total = np.sum(n)

    ## initialization
    x = np.zeros((n_total, p))
    y = np.zeros(n_total)

    # calculate sample indices for each task
    task_range = []
    start_index = 0
    for t in range(T):
        task_range.append(range(start_index, start_index+n[t]))
        start_index += n[t]

    # stack the x and y arrays
    for t in range(T):
        x[task_range[t], :] = data[t][0]
        y[task_range[t]] = data[t][1]

    M = (x.T @ np.diag(y**2) @ x)/n_total

    # SVD
    A_hat = np.linalg.svd(M)[0][:, 0:r]

    # calculate the theta estimate based on A_hat
    theta_hat = np.zeros((r, T))
    for t in range(T):
        theta_hat[:, t] = LinearRegression(fit_intercept = False).fit(x[task_range[t], :] @ A_hat, y[task_range[t]]).coef_

    # calculate the estimate of coef
    beta_hat = np.zeros((p, T))
    for t in range(T):
        beta_hat[:, t] = A_hat @ theta_hat[:, t]

    return(beta_hat)

"""# GeoERM

"""

# Implementation of different MTL methods
import numpy as np
import torch
import torch.optim as optim
from sklearn.linear_model import LinearRegression, LogisticRegression
import logging

# QR retraction
# def R(X, G):
#     Q, _ = torch.qr(X + G)
#     return Q
def R(X):
    Q, _ = torch.qr(X)
    return Q

# Polar retration
# def polar_retraction(X, G):
#     U, _, Vt = torch.linalg.svd(X + G, full_matrices=False)
#     return U @ Vt
def polar_retraction(X):
    U, _, Vt = torch.linalg.svd(X, full_matrices=False)
    return U @ Vt

# Orthogonal Projection1
def Proj(X, U):
    print(X.shape)
    print(U.shape)
    return U - X @ (X.T @ U + U.T @ X) / 2

# Orthogonal Projection2
def projection(X, U):
    # X^T * U
    XtU = torch.matmul(X.transpose(-2, -1), U)
    symXtU = (XtU + XtU.transpose(-2, -1)) / 2

    # Projection: U - X * symXtU
    Up = U - torch.matmul(X, symXtU)
    return Up

def column_norm(A):
    norm_A = np.zeros(A.shape[1])
    for j in range(A.shape[1]):
        norm_A[j] = np.linalg.norm(A[:, j])
    return(norm_A)

# def initialize_stiefel_matrix(p, r):

#     random_matrix = torch.randn(p, r, dtype=torch.float64)
#     Q, _ = torch.linalg.qr(random_matrix)
#     return Q[:, :r].requires_grad_()

# def initialize_stiefel_tensor(T, p, r):

#     A_hat = torch.zeros((T, p, r), dtype=torch.float64)
#     for t in range(T):
#         random_matrix = torch.randn(p, r, dtype=torch.float64)
#         Q, _ = torch.linalg.qr(random_matrix)
#         A_hat[t] = Q[:, :r]
#     return A_hat.requires_grad_()

def initialize_stiefel_matrix(p, r):
    random_matrix = np.random.randn(p, r)
    Q, _ = np.linalg.qr(random_matrix)
    return Q[:, :r]

def initialize_stiefel_tensor(T, p, r):
    A_hat = np.zeros((T, p, r))
    for t in range(T):
        random_matrix = np.random.randn(p, r)
        Q, _ = np.linalg.qr(random_matrix)
        A_hat[t] = Q[:, :r]
    return A_hat

## penalized GeoERM
def GeoERM(data, r = 3, T1 = 1, T2 = 1, R = None, r_bar = None, lr = 0.01, max_iter = 2000, C1 = 1, C2 = 1,
            delta = 0.05, adaptive = False, info = False, tol = 1e-6, link = 'linear'):
    if info:
        print("pERM starts running...", flush = True)

    T = len(data)
    n = np.array([x.shape[0] for (x,y) in data])
    p = data[0][0].shape[1]
    n_total = np.sum(n)

    ## initialization
    x = np.zeros((n_total, p))
    y = np.zeros(n_total)

    # calculate sample indices for each task
    task_range = []
    start_index = 0
    for t in range(T):
        task_range.append(range(start_index, start_index+n[t]))
        start_index += n[t]

    # stack the x and y arrays
    for t in range(T):
        x[task_range[t], :] = data[t][0]
        y[task_range[t]] = data[t][1]

    ## r adaptive or not
    if (adaptive == True):
        # single-task linear regression
        beta_hat_single_task = np.zeros((p, T))
        if link == 'linear':
            for t in range(T):
                beta_hat_single_task[:, t] = LinearRegression(fit_intercept = False).fit(x[task_range[t], :], y[task_range[t]]).coef_
        elif link == 'logistic':
            for t in range(T):
                beta_hat_single_task[:, t] = LogisticRegression(fit_intercept = False).fit(x[task_range[t], :], y[task_range[t]]).coef_

        norm_each_task = column_norm(beta_hat_single_task)
        if R is None:
            R = np.median(norm_each_task)*2

        for t in range(T):
            if (norm_each_task[t] > R):
                beta_hat_single_task[:, t] = beta_hat_single_task[:, t]/norm_each_task[t]*R

        # set up threshold
        if r_bar is None:
            r_bar = x.shape[1]
        threshold = T1*np.sqrt((p+np.log(T))/np.max(n)) + T2*R*(r_bar**(-3/4))
        r = max(np.where(np.linalg.svd(beta_hat_single_task/np.sqrt(T))[1] > threshold)[0])+1
        if info:
            print('Selected r = ' + str(r))

    # initialization
    y = torch.tensor(y, requires_grad=False)
    x = torch.tensor(x, requires_grad=False)
    # A_hat = np.random.rand(T, p, r)#, dtype ='float64')
    # A_bar = np.random.rand(p, r)#, dtype='float64')
    A_hat = initialize_stiefel_tensor(T, p, r)
    A_bar = initialize_stiefel_matrix(p, r)
    A_bar[0:r, 0:r] = np.identity(r,dtype='float64')

    for t in range(T):
        A_hat[t, 0:r, 0:r] = np.identity(r)

    theta_hat = np.zeros((r, T))

    # transform arrays to tensors
    # A_hat = torch.tensor(A_hat, requires_grad=True)
    A_bar = torch.tensor(A_bar, requires_grad=True, dtype=torch.float64)
    A_hat = torch.tensor(A_hat, requires_grad=True, dtype=torch.float64)
    theta_hat = torch.tensor(theta_hat, requires_grad=True, dtype=torch.float64)

    ## Step 1
    lam = np.sqrt(r*(p+np.log(T)))*C1

    if link == 'linear':
        def ftotal(A, theta, A_bar):
            s = 0
            for t in range(T):
                s = s + 1/(2*n_total)*torch.dot(y[task_range[t]] - x[task_range[t], :] @ A[t, :, :] @ theta[:, t], y[task_range[t]]
                                     - x[task_range[t], :] @ A[t, :, :] @ theta[:, t]) + lam*np.sqrt(n[t])/n_total*torch.linalg.svd(A[t, :, :] @ torch.linalg.inv(A[t, :, :].T @ A[t, :, :]) @ A[t, :, :].T - A_bar @ torch.linalg.inv(A_bar.T @ A_bar) @ A_bar.T)[1][0]
            return(s)
    elif link == 'logistic':
        def ftotal(A, theta, A_bar):
            s = 0
            for t in range(T):
                logits = torch.matmul(x[task_range[t], :], A[t, :, :] @ theta[:, t])
                s = s + 1/n_total*torch.dot(1-y[task_range[t]], logits) + 1/n_total*torch.sum(torch.log(1+torch.exp(-logits))) + lam*np.sqrt(n[t])/n_total*torch.linalg.svd(A[t, :, :] @ torch.linalg.inv(A[t, :, :].T @ A[t, :, :]) @ A[t, :, :].T - A_bar @ torch.linalg.inv(A_bar.T @ A_bar) @ A_bar.T)[1][0]

            return(s)

    optimizer = optim.Adam([A_hat, theta_hat, A_bar], lr=lr)

    loss_last = 1e8
    for i in range(max_iter):
        # Zero the gradients
        optimizer.zero_grad()

        # Compute the loss (sum of the largest singular values)
        loss = ftotal(A_hat, theta_hat, A_bar)

        # Backward pass to compute gradients
        loss.backward()
        # print(A_bar.grad)

        # Projection on gradients
        projected_A_hat_grad = projection(A_hat, A_hat.grad)
        projected_A_bar_grad = projection(A_bar, A_bar.grad)

        # Update the gradients to their projected versions
        with torch.no_grad():
            A_hat.grad.copy_(projected_A_hat_grad)
            A_bar.grad.copy_(projected_A_bar_grad)

        # Update the matrices with Adam
        optimizer.step()
        # print(A_bar.grad)

        with torch.no_grad():
            A_hat.copy_(polar_retraction(A_hat))
            A_bar.copy_(polar_retraction(A_bar))
        # with torch.no_grad():
        #     A_hat.copy_(R(A_hat))
        #     A_bar.copy_(R(A_bar))

        # Loss
        if info:
            if (i + 1) % 100 == 0:
                print("Iteration {}/{}, Loss: {}".format(i+1, max_iter, loss.item()), flush=True)

        # Early Stop
        if abs(loss_last - loss.item()) / loss.item() <= tol:
            if info:
                print("Already converged. Stopped early.", flush=True)
            break
        loss_last = loss.item()

    beta_hat_step1 = torch.zeros(p, T, dtype=torch.float64)
    for t in range(T):
        beta_hat_step1[:, t] = A_hat[t, :, :] @ theta_hat[:, t]

    beta_hat_step1 = beta_hat_step1.detach()
    if info:
        print("Step 1 is completed.\n", flush=True)

    ## Step 2
    gamma = np.sqrt(p+np.log(T))*C2
    beta = torch.zeros(p, T, requires_grad = True, dtype=torch.float64)

    if link == 'linear':
        def ftotal2(beta):
            s = 0
            for t in range(T):
                s = s + 1/(2*n[t])*torch.dot(y[task_range[t]] - x[task_range[t], :] @ beta[:, t], y[task_range[t]] - x[task_range[t], :] @ beta[:, t]) + gamma/np.sqrt(n[t])*torch.norm(beta[:, t] - beta_hat_step1[:, t])
            return(s)
    elif link == 'logistic':
        def ftotal2(beta):
            s = 0
            for t in range(T):
                logits = torch.matmul(x[task_range[t], :], beta[:, t])
                s = s + 1/n[t]*torch.dot(1-y[task_range[t]], logits) + 1/n[t]*torch.sum(torch.log(1+torch.exp(-logits))) + gamma/np.sqrt(n[t])*torch.norm(beta[:, t] - beta_hat_step1[:, t])
            return(s)


    optimizer2 = optim.Adam([beta], lr=lr)
    loss_last = 1e8
    for i in range(max_iter):
        # Zero the gradients
        optimizer2.zero_grad()

        # Compute the loss (sum of the largest singular values)
        loss2 = ftotal2(beta)

        # Backward pass to compute gradients
        loss2.backward()

        # Update the matrices
        optimizer2.step()

        # Print the loss every 100 iterations
        if info:
            if (i + 1) % 100 == 0:
                print("Iteration {}/{}, Loss: {}".format(i+1, max_iter, loss2.item()), flush = True)
        if abs(loss_last-loss2.item())/loss2.item() <= tol:
            if info:
                print("Already converged. Stopped early.", flush = True)
            break
        loss_last = loss2.item()

    beta_hat_step1 = beta_hat_step1.numpy()
    beta_hat_step2 = beta.detach().numpy()
    if info:
        print("Step 2 is completed.\n", flush = True)

    if info:
        print("GeoERM stops running...", flush = True)

    return({"step1": beta_hat_step1, "step2": beta_hat_step2})

"""# 9 Models Linear Setup

Updated Setup with iteration
"""

from joblib import Parallel, delayed
import numpy as np


# Helper function to calculate MSE
def calculate_mse(model, train_data, true_beta):
    if model == 'GeoERM':
        result = GeoERM(train_data, r=3, link='linear')
        beta_hat = result['step2']
    elif model == 'pERM':
        result = pERM(train_data, r=3, link='linear')
        beta_hat = result['step2'] if isinstance(result, dict) else result
    elif model == 'ERM':
        result = ERM(train_data, r=3, link='linear')
    elif model == 'single_task_LR':
        result = single_task_LR(train_data, link='linear')
    elif model == 'pooled_LR':
        result = pooled_LR(train_data, link='linear')
    elif model == 'spectral':
        result = spectral(train_data, r=3, link='linear')
    elif model == 'MoM':
        result = MoM(train_data, r=3)
    elif model == 'AdaptRep':
        result = AdaptRep(train_data, r=3)
    elif model == 'GLasso':
        result = spectral(train_data, r=3, link='linear')
    else:
        raise ValueError("Unknown model")

    # Compare the model's beta_hat with the true beta
    # beta_hat = result if isinstance(result, np.ndarray) else result['step2']
    # mse = np.mean((beta_hat - true_beta)**2)
    # Compare the model's beta_hat with the true beta
    beta_hat = result if isinstance(result, np.ndarray) else result['step2']
    mse = max_distance(beta_hat, true_beta)
    return mse

def main_execution_function(n_tasks=50, n_samples=100, n_features=50, r=3, epsilon=0.1, h_list=np.arange(0.1, 0.9, 0.1), n_iterations=1):
    # Initialize total results to accumulate over iterations
    total_results = {h: {"no_outliers": {model: [] for model in ['GeoERM', 'pERM', 'ERM', 'single_task_LR', 'pooled_LR', 'spectral', 'MoM', 'AdaptRep', 'GLasso']},
                         "with_outliers": {model: [] for model in ['GeoERM', 'pERM', 'ERM', 'single_task_LR', 'pooled_LR', 'spectral', 'MoM', 'AdaptRep', 'GLasso']}}
                     for h in h_list}

    # Loop over each iteration
    for iteration in range(n_iterations):
        print(f"Iteration {iteration + 1}/{n_iterations}...")

        # Loop over each h
        for h in h_list:
            print(f"Running for h = {h}...")

            # Generate datasets with and without outliers, for each h
            data_no_outliers = generate_data(n=n_samples, p=n_features, r=r, T=n_tasks, epsilon=0, h=h, link='linear')
            data_with_outliers = generate_data(n=n_samples, p=n_features, r=r, T=n_tasks, epsilon=epsilon, h=h, link='linear')

            # Models
            models = ['GeoERM', 'pERM', 'ERM', 'single_task_LR', 'pooled_LR', 'spectral', 'MoM', 'AdaptRep', 'GLasso']

            # Sequential execution
            mse_results_no_outliers = []
            mse_results_with_outliers = []

            for model in models:
                mse_no_outlier = calculate_mse(model, data_no_outliers['data'], data_no_outliers['beta'])
                mse_with_outlier = calculate_mse(model, data_with_outliers['data'], data_with_outliers['beta'])
                mse_results_no_outliers.append(mse_no_outlier)
                mse_results_with_outliers.append(mse_with_outlier)

            # Accumulate the results
            for model, mse_no_outlier, mse_with_outlier in zip(models, mse_results_no_outliers, mse_results_with_outliers):
                total_results[h]["no_outliers"][model].append(mse_no_outlier)
                total_results[h]["with_outliers"][model].append(mse_with_outlier)

    # After all iterations, compute the average and standard error
    averaged_results = {h: {"no_outliers": {model: np.mean(total_results[h]["no_outliers"][model]) for model in models},
                            "with_outliers": {model: np.mean(total_results[h]["with_outliers"][model]) for model in models}}
                        for h in h_list}

    standard_errors = {h: {"no_outliers": {model: np.std(total_results[h]["no_outliers"][model], ddof=1) / np.sqrt(n_iterations) for model in models},
                           "with_outliers": {model: np.std(total_results[h]["with_outliers"][model], ddof=1) / np.sqrt(n_iterations) for model in models}}
                       for h in h_list}

    # Print averaged results with standard error
    for h in h_list:
        print(f"Averaged MSE results (no outliers, h={h}):", averaged_results[h]["no_outliers"])
        print(f"Standard Errors (no outliers, h={h}):", standard_errors[h]["no_outliers"])
        print(f"Averaged MSE results (with outliers, h={h}):", averaged_results[h]["with_outliers"])
        print(f"Standard Errors (with outliers, h={h}):", standard_errors[h]["with_outliers"])

        # Save averaged_results and standard_errors as JSON files
#     import json
# # Save the results to unique JSON files using the SLURM_PROCID or a custom identifier
#     task_id = os.environ.get("SLURM_PROCID", "0")  # Use SLURM_PROCID if available, default to "0"
#     with open(f'averaged_results_{task_id}.json', 'w') as f:
#         json.dump(averaged_results, f, indent=4)
#     with open(f'standard_errors_{task_id}.json', 'w') as f:
#         json.dump(standard_errors, f, indent=4)

#     return averaged_results, standard_errors

    import os
    import json
    import sys

    output_dir = ''
    os.makedirs(output_dir, exist_ok=True)

    task_id = os.environ.get("SLURM_ARRAY_TASK_ID", "0")  # 
    averaged_file_path = os.path.join(output_dir, f'averaged_results_{task_id}.json')
    standard_errors_file_path = os.path.join(output_dir, f'standard_errors_{task_id}.json')

    if os.path.exists(averaged_file_path) or os.path.exists(standard_errors_file_path):
        print(f"File {averaged_file_path} or {standard_errors_file_path} exists. Stopping the script.", flush=True)
        sys.exit(0)
    else:
        print(f"Files do not exist. Continuing the script.", flush=True)

    with open(averaged_file_path, 'w') as f:
        json.dump(averaged_results, f, indent=4)
    with open(standard_errors_file_path, 'w') as f:
        json.dump(standard_errors, f, indent=4)

    print(f"Results saved to {averaged_file_path} and {standard_errors_file_path}", flush=True)

    return averaged_results, standard_errors


"""# 9 Models Linear Result

"""
if __name__ == "__main__":
    models = ['GeoERM', 'pERM', 'ERM', 'single_task_LR', 'pooled_LR', 'spectral', 'MoM', 'AdaptRep', 'GLasso']
    logging.basicConfig(level=logging.INFO)

    # Call main_execution_function to get both averaged results and standard errors
    averaged_results, standard_errors = main_execution_function(n_iterations=1)
